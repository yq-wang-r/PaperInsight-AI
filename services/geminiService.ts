import { GoogleGenAI, Tool } from "@google/genai";
import { AnalysisResult, ChatMessage } from "../types";

const SYSTEM_INSTRUCTION = `
Role: ä½ æ˜¯ä¸€ä½è®¡ç®—æœºé¢†åŸŸçš„èµ„æ·±ç§‘ç ”åŠ©æ‰‹ï¼Œæ“…é•¿å¿«é€Ÿè§£æå­¦æœ¯è®ºæ–‡å¹¶æå–æ ¸å¿ƒé€»è¾‘ã€‚
Task: è¯·å¸®æˆ‘æ£€ç´¢å¹¶é˜…è¯»æŒ‡å®šçš„è®¡ç®—æœºé¢†åŸŸæ–‡ç« /è¯é¢˜ã€‚ä½ çš„ç›®æ ‡æ˜¯ä¸ä»…æ€»ç»“åŸæ–‡ï¼Œè¿˜è¦ä»¥å®¡è¾©å¼æ€ç»´è¾…åŠ©æˆ‘è¿›è¡Œç§‘ç ”æ€è€ƒã€‚
Output Format: è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ Markdown æ ¼å¼è¾“å‡ºï¼Œä¸è¦è¾“å‡ºMarkdownä»£ç å—æ ‡è®°ï¼ˆå¦‚ \`\`\`markdownï¼‰ï¼Œç›´æ¥è¾“å‡ºå†…å®¹ã€‚

ğŸ“„ è®ºæ–‡æ¦‚è§ˆ
æ ‡é¢˜: [æ–‡ç« æ ‡é¢˜]
ä½œè€…: [ä¸»è¦ä½œè€…å§“å]
å‘è¡¨å¹´ä»½/ä¼šè®®/æœŸåˆŠ: [ä¾‹å¦‚ï¼š2024 / IEEE INFOCOM]
é“¾æ¥: [arXiv/DOI é“¾æ¥]

ğŸ” æ ¸å¿ƒå†…å®¹
ç ”ç©¶é—®é¢˜: [é’ˆå¯¹ä»€ä¹ˆå…·ä½“çš„ç—›ç‚¹æˆ–æŒ‘æˆ˜ï¼Ÿï¼ˆç”¨ 1-2 å¥è¯æ¦‚æ‹¬ï¼‰]
ä¸»è¦æ–¹æ³•: [æå‡ºäº†ä»€ä¹ˆæ ·çš„ç®—æ³•ã€æ¶æ„æˆ–ç†è®ºè¯æ˜ï¼Ÿ]
å…³é”®è´¡çŒ®:
1. [è´¡çŒ® 1]
2. [è´¡çŒ® 2]
3. [è´¡çŒ® 3]

ğŸ’¡ å¯å‘ä¸æ€è€ƒ
äº®ç‚¹: [è®ºæ–‡æœ€ç²¾å¦™çš„è®¾è®¡æˆ–æœ€ä»¤äººä¿¡æœçš„å®éªŒç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ]
ä¸è¶³: [å®éªŒè®¾ç½®ã€å‡è®¾å‰ææˆ–æ‰©å±•æ€§ä¸Šæ˜¯å¦å­˜åœ¨å±€é™æ€§ï¼Ÿ]
å¯å€Ÿé‰´ç‚¹: [å…¶ä¸­çš„å“ªäº›æŠ€æœ¯è·¯å¾„ã€è¯„ä¼°æŒ‡æ ‡æˆ–æ•°å­¦å·¥å…·å¯ä»¥è¿ç§»åˆ°å…¶ä»–ç ”ç©¶ä¸­ï¼Ÿè¯·ç»™å‡ºå…·ä½“ã€å¯æ“ä½œçš„è¿ç§»å»ºè®®ï¼Œå¹¶ç»“åˆæ½œåœ¨åº”ç”¨åœºæ™¯æä¾›ç¤ºä¾‹ã€‚]
å¾…è§£å†³é—®é¢˜: [è®ºæ–‡æåˆ°çš„æœªæ¥æ–¹å‘æˆ–ä½ è§‚å¯Ÿåˆ°çš„æœªç«Ÿä¹‹å¿—ã€‚]
å¤‡æ³¨: [ç»“åˆå½“å‰è®¡ç®—æœºé¢†åŸŸçš„æŠ€æœ¯è¶‹åŠ¿ï¼ˆå¦‚å¤§æ¨¡å‹ã€è¾¹ç¼˜è®¡ç®—ç­‰ï¼‰ç»™å‡ºæ·±åº¦çš„ä¸“ä¸šè¯„ä»·ã€‚]
`;

export const analyzePaperWithGemini = async (query: string): Promise<AnalysisResult> => {
  if (!process.env.API_KEY) {
    throw new Error("API Key is missing. Please set it in the environment.");
  }

  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

  // Use Google Search to find the paper and details
  const tools: Tool[] = [
    { googleSearch: {} }
  ];

  try {
    const response = await ai.models.generateContent({
      model: 'gemini-3-pro-preview', // Using Pro for better reasoning and analysis
      contents: `Search for and analyze the paper related to: "${query}". If multiple papers match, choose the most relevant or influential one. Strictly follow the defined output format.`,
      config: {
        systemInstruction: SYSTEM_INSTRUCTION,
        tools: tools,
        temperature: 0.3, // Lower temperature for more factual extraction
      }
    });

    const text = response.text || "Analysis generation failed.";
    const groundingChunks = response.candidates?.[0]?.groundingMetadata?.groundingChunks as any[];

    return {
      markdown: text,
      groundingChunks: groundingChunks
    };
  } catch (error) {
    console.error("Gemini API Error:", error);
    throw error;
  }
};

export const askFollowUp = async (
  question: string, 
  originalContext: string, 
  history: ChatMessage[]
): Promise<string> => {
  if (!process.env.API_KEY) {
    throw new Error("API Key is missing.");
  }

  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

  // Construct context from history
  const historyContext = history.map(msg => `${msg.role === 'user' ? 'User' : 'Assistant'}: ${msg.content}`).join('\n');

  const prompt = `
    Context (Original Analysis):
    ${originalContext}

    Conversation History:
    ${historyContext}

    Current User Question:
    ${question}

    INTERNAL INSTRUCTION (CRITICAL):
    You must perform an internal critique before answering. 
    1. Draft an initial answer based on the paper analysis and your knowledge.
    2. Critique your draft: Does it directly address the user's specific question? Is the tone professional and academic? Is it accurate?
    3. Refine the answer based on the critique.
    4. **IMPORTANT**: Do not use Markdown bolding (i.e., do not use **asterisks**) in the final answer. Keep it plain text for easier reading.
    
    OUTPUT FORMAT:
    You must wrap your FINAL, polished answer inside <final_answer> tags. Do not show the critique process to the user, only the result inside the tags.
    
    Example:
    <final_answer>
    The paper utilizes a Transformer architecture...
    </final_answer>
  `;

  try {
    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview', // Flash is sufficient for chat context
      contents: prompt,
      config: {
        temperature: 0.5,
      }
    });

    const rawText = response.text || "";
    
    // Extract content within <final_answer> tags and clean up markdown bolding
    const match = rawText.match(/<final_answer>([\s\S]*?)<\/final_answer>/);
    
    if (match && match[1]) {
      return match[1].trim().replace(/\*\*/g, '');
    } else {
      // Fallback if model fails to tag (rare with high temp, but possible)
      return rawText.replace(/<final_answer>|<\/final_answer>/g, '').replace(/\*\*/g, '').trim();
    }

  } catch (error) {
    console.error("Follow-up Error:", error);
    throw error;
  }
};